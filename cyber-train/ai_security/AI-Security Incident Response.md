# AI Security Incident Response Workflow

## Document Information

**Version**: 1.0  
**Date**: October 22, 2025  
**Sources**: 
- OWASP GenAI COMPASS Play Book v1.0 (July 4, 2025)
- OWASP GenAI COMPASS Framework
- Current AI Security Incident Intelligence (October 2025)
- AI Incident Database (https://incidentdatabase.ai/)
- CISA Federal Playbooks (adapted for AI/ML systems)

**Classification**: TLP:CLEAR  
**Purpose**: Comprehensive operational procedures for planning and conducting AI/ML security incident response activities

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [AI Threat Landscape (October 2025)](#ai-threat-landscape-october-2025)
3. [AI Security Incident Response Framework](#ai-security-incident-response-framework)
4. [Phase 1: Preparation for AI Incidents](#phase-1-preparation-for-ai-incidents)
5. [Phase 2: Detection & Analysis of AI Incidents](#phase-2-detection--analysis-of-ai-incidents)
6. [Phase 3: Containment of AI Incidents](#phase-3-containment-of-ai-incidents)
7. [Phase 4: Eradication & Recovery](#phase-4-eradication--recovery)
8. [Phase 5: Post-Incident Activities](#phase-5-post-incident-activities)
9. [AI-Specific Incident Categories](#ai-specific-incident-categories)
10. [Integration with Standard IR Workflow](#integration-with-standard-ir-workflow)

---

## Executive Summary

The rapid adoption of artificial intelligence and machine learning systems has introduced a new category of cybersecurity incidents requiring specialized response procedures. This workflow integrates the **OWASP GenAI COMPASS OODA Loop methodology** (Observe, Orient, Decide, Act) with traditional NIST SP 800-61 incident response practices to address the unique challenges of AI security incidents.

### Current AI Threat Environment (October 2025)

The AI security landscape has evolved dramatically, with several critical trends emerging:

**Attack Volume and Sophistication**
- **16% of cyber incidents** now involve attackers leveraging AI capabilities
- **442% increase** in voice phishing (vishing) attacks in H2 2024 (CrowdStrike data)
- **37% of AI-involved breaches** utilize AI-generated phishing
- **35% of AI-involved breaches** employ deepfake impersonation attacks
- **40% of AI data breaches** predicted to arise from cross-border GenAI misuse by 2027 (Gartner)

**Emerging Attack Vectors**
- AI-generated deepfake extortion targeting minors and vulnerable populations
- Autonomous LLM-based attacks capable of compromising networks without human intervention
- Prompt injection and jailbreaking attacks against production AI systems
- Model inversion attacks extracting training data
- Supply chain attacks targeting AI model repositories
- Shadow AI creating unmanaged security exposures

**Recent Notable Incidents (2024-2025)**
- **Incident 1241**: AI-generated deepfake sextortion via Telegram (RM5,800 extortion attempt)
- **Incident 1232**: Automated driving system failure with fatal consequences
- **Incident 1231**: Political deepfake in campaign advertising
- **Multiple incidents**: LLM hijacking attacks using stolen cloud credentials
- **Research findings**: LLMs fully compromising 5 of 10 test networks autonomously (Anthropic)

### Unique Characteristics of AI Security Incidents

AI security incidents differ from traditional cybersecurity incidents in several critical ways:

1. **Dual Nature**: Incidents may involve AI as the **attack vector** (AI-powered attacks) or as the **attack target** (compromised AI systems)
2. **Opacity**: AI system behavior may be difficult to interpret, making incident detection and analysis challenging
3. **Data Sensitivity**: AI systems often process vast amounts of sensitive data, increasing breach impact
4. **Rapid Evolution**: AI threats evolve faster than traditional threats due to continuous model updates
5. **Cross-Domain Impact**: AI incidents may affect safety, privacy, ethics, and security simultaneously
6. **Attribution Challenges**: Distinguishing between model errors, adversarial attacks, and legitimate use is complex

---

## AI Threat Landscape (October 2025)

### Threat Profile Classification (OWASP COMPASS Framework)

The OWASP GenAI COMPASS framework organizes AI threats into three primary profiles based on organizational perspective:

#### Profile 1: External Adversary Using AI

**Description**: Threat actors leveraging AI tools to accelerate and enhance attacks against the organization

**Current Threat Vectors**:

1. **AI-Generated Phishing and Social Engineering**
   - Highly convincing spear-phishing emails generated by LLMs
   - Voice cloning for vishing attacks (442% increase in H2 2024)
   - Deepfake video calls impersonating executives
   - AI-generated malicious code and exploits
   - Automated vulnerability discovery and exploitation

2. **Deepfake-Based Attacks**
   - Executive impersonation for financial fraud
   - Synthetic identity creation for account takeover
   - Deepfake extortion (as seen in Incident 1241)
   - Manipulated media for disinformation campaigns
   - Fake biometric authentication bypass

3. **Autonomous AI Attackers**
   - LLM-powered autonomous penetration testing
   - Self-directed lateral movement and privilege escalation
   - Adaptive evasion of security controls
   - Automated data exfiltration and encryption
   - AI-driven command and control (C2) systems

4. **AI-Enhanced Reconnaissance**
   - Automated OSINT collection and analysis
   - Pattern recognition for vulnerability identification
   - Social media profiling for targeted attacks
   - Automated password cracking and credential stuffing
   - Network mapping and attack surface analysis

**Recent Examples**:
- **Vishing surge**: 442% increase in AI-powered voice phishing
- **Autonomous attacks**: LLMs compromising 5/10 networks without human guidance
- **Deepfake extortion**: Malaysian minor targeted with AI-generated explicit content

---

#### Profile 2: Deployer (Model User) Risks

**Description**: Organizations using AI systems in their operations face application-level risks related to deployment and impact on users

**Current Threat Vectors**:

1. **Prompt Injection Attacks**
   - Direct prompt injection manipulating model outputs
   - Indirect prompt injection via poisoned data sources
   - Jailbreaking to bypass safety guardrails
   - Prompt leaking to extract system instructions
   - Cross-prompt injection in multi-user systems

2. **Data Leakage and Privacy Violations**
   - Training data extraction through model inversion
   - Sensitive information disclosure in model outputs
   - Membership inference attacks
   - Unintended memorization of PII
   - Cross-tenant data leakage in shared deployments

3. **Model Manipulation and Abuse**
   - Adversarial inputs causing misclassification
   - Model poisoning through feedback mechanisms
   - Resource exhaustion via expensive queries
   - Denial of service through model overload
   - Abuse for generating harmful content

4. **Shadow AI and Unmanaged Deployments**
   - Employees using unauthorized AI tools
   - Sensitive data exposure to third-party AI services
   - Lack of governance and compliance controls
   - Untracked AI-related security exposures
   - Inconsistent security policies across AI tools

5. **Integration and API Security**
   - Insecure AI API authentication
   - Excessive permissions granted to AI systems
   - Lack of input validation on AI inputs/outputs
   - Vulnerable AI service integrations
   - Insufficient rate limiting and abuse prevention

**Recent Examples**:
- **Shadow AI impact**: Increased cost of data breaches when unmanaged AI tools are used
- **LLM hijacking**: Attackers using stolen cloud credentials to abuse hosted LLM services
- **Flowbreaking exploits**: Triggering data leaks from production AI systems

---

#### Profile 3: Provider (Model Builder) Risks

**Description**: Organizations developing and deploying AI models face core model behavior and systemic risks

**Current Threat Vectors**:

1. **Model Supply Chain Attacks**
   - Compromised pre-trained models from repositories
   - Poisoned training datasets
   - Malicious model weights and parameters
   - Backdoored ML frameworks and libraries
   - Compromised model serving infrastructure

2. **Training Data Poisoning**
   - Adversarial data injection during training
   - Backdoor insertion via poisoned samples
   - Bias amplification through targeted poisoning
   - Data integrity attacks on training pipelines
   - Federated learning poisoning attacks

3. **Model Theft and Intellectual Property**
   - Model extraction through API queries
   - Architecture reverse engineering
   - Knowledge distillation attacks
   - Unauthorized model replication
   - Theft of proprietary training data

4. **Infrastructure and Deployment Security**
   - Compromised ML training infrastructure
   - Insecure model storage and versioning
   - Vulnerable model serving endpoints
   - Insufficient access controls on model artifacts
   - Inadequate monitoring of model behavior

5. **Adversarial Machine Learning**
   - Evasion attacks against deployed models
   - Poisoning attacks during retraining
   - Model inversion to extract training data
   - Membership inference attacks
   - Byzantine attacks in distributed training

**Recent Examples**:
- **Model compromise**: Concerns about supply chain security in AI model repositories
- **Data poisoning**: Demonstrated attacks on public AI training datasets
- **Model extraction**: Successful theft of commercial model capabilities

---

### AI Attack Surface Analysis

The AI attack surface extends across multiple layers:

**Layer 1: Data Layer**
- Training data repositories
- Data labeling and annotation systems
- Data preprocessing pipelines
- Feature stores and data warehouses
- Real-time data ingestion streams

**Layer 2: Model Layer**
- Model training infrastructure
- Model repositories and registries
- Model serving endpoints
- Model versioning and deployment systems
- Fine-tuning and adaptation pipelines

**Layer 3: Application Layer**
- AI-powered applications and services
- API endpoints for AI functionality
- User interfaces for AI interactions
- Integration points with other systems
- Feedback and learning mechanisms

**Layer 4: Infrastructure Layer**
- Cloud AI services (AWS SageMaker, Azure ML, Google Vertex AI)
- GPU/TPU compute resources
- Container orchestration (Kubernetes)
- Model serving infrastructure
- Monitoring and logging systems

**Layer 5: Governance Layer**
- AI policy and compliance frameworks
- Access control and authentication
- Audit logging and monitoring
- Privacy and data protection controls
- Model risk management processes

---

## AI Security Incident Response Framework

The AI Security Incident Response Framework adapts the traditional NIST SP 800-61 phases with AI-specific considerations and integrates the OWASP COMPASS OODA Loop for continuous threat assessment.

### Framework Integration: NIST + OODA Loop

**Traditional NIST Phases** → **OODA Loop Integration**

1. **Preparation** → **Observe**: Establish AI threat monitoring and baseline behavior
2. **Detection & Analysis** → **Observe + Orient**: Detect AI incidents and assess using threat intelligence
3. **Containment** → **Decide**: Determine containment strategy based on AI-specific risks
4. **Eradication & Recovery** → **Act**: Execute remediation and restore AI services
5. **Post-Incident** → **Observe**: Feed lessons learned back into continuous improvement

### AI Incident Severity Classification

**Critical (AI-S1)**
- Autonomous AI system causing physical harm or safety risk
- Large-scale data breach from AI system (>100K records)
- AI system manipulation affecting critical infrastructure
- Widespread deepfake campaign causing significant harm
- Compromised AI model with national security implications
- AI-powered attack causing major operational disruption

**High (AI-S2)**
- Significant AI model manipulation or poisoning
- Moderate data leakage from AI system (10K-100K records)
- Successful prompt injection with sensitive data exposure
- Deepfake attack targeting executives or public figures
- Shadow AI tool exposing confidential information
- AI service disruption affecting critical business functions

**Medium (AI-S3)**
- Limited AI model manipulation without data exposure
- Minor data leakage from AI system (<10K records)
- Attempted but unsuccessful prompt injection
- Detected adversarial inputs to AI systems
- Policy violations in AI usage
- Performance degradation of AI services

**Low (AI-S4)**
- Suspicious AI system behavior without confirmed compromise
- Attempted attacks against AI systems with no impact
- Minor policy violations in AI tool usage
- Anomalous AI outputs requiring investigation

---

## Phase 1: Preparation for AI Incidents

### 1.1 AI Asset Inventory and Classification

**Objective**: Establish comprehensive inventory of all AI/ML assets and their risk profiles

**Exact Steps**:

1. **Inventory AI Systems and Models**
   - Step 1.1.1: Identify all AI/ML models in production
   - Step 1.1.2: Document model types (LLM, computer vision, predictive, etc.)
   - Step 1.1.3: Identify model sources (in-house, third-party, open-source)
   - Step 1.1.4: Document model versions and update frequency
   - Step 1.1.5: Identify model hosting locations (cloud, on-prem, edge)
   - Step 1.1.6: Map models to business functions and data flows

2. **Classify AI Systems by Risk**
   - Step 1.1.7: Assess criticality of each AI system to operations
   - Step 1.1.8: Evaluate sensitivity of data processed by AI systems
   - Step 1.1.9: Assess potential impact of AI system compromise
   - Step 1.1.10: Classify AI systems using OWASP COMPASS profiles
   - Step 1.1.11: Assign risk ratings to each AI system
   - Step 1.1.12: Prioritize AI systems for security monitoring

3. **Document AI System Dependencies**
   - Step 1.1.13: Map AI system dependencies on data sources
   - Step 1.1.14: Identify upstream and downstream system integrations
   - Step 1.1.15: Document API dependencies and third-party services
   - Step 1.1.16: Identify infrastructure dependencies (GPU, storage, etc.)
   - Step 1.1.17: Map human dependencies and approval workflows
   - Step 1.1.18: Create AI system dependency diagrams

4. **Identify Shadow AI**
   - Step 1.1.19: Survey employees on AI tool usage
   - Step 1.1.20: Monitor network traffic for AI service connections
   - Step 1.1.21: Review cloud service usage for unauthorized AI tools
   - Step 1.1.22: Analyze browser extensions and desktop applications
   - Step 1.1.23: Document discovered shadow AI instances
   - Step 1.1.24: Assess risk of shadow AI tools

**Deliverables**:
- AI asset inventory database
- AI system risk classifications
- AI dependency maps
- Shadow AI assessment report

**Success Criteria**:
- Complete inventory of all AI systems
- Risk classification for each AI asset
- Dependencies documented
- Shadow AI identified and assessed

---

### 1.2 AI-Specific Detection and Monitoring

**Objective**: Deploy specialized monitoring for AI security threats

**Exact Steps**:

1. **Deploy AI Model Monitoring**
   - Step 1.2.1: Implement model performance monitoring
   - Step 1.2.2: Configure model drift detection
   - Step 1.2.3: Deploy data drift monitoring
   - Step 1.2.4: Implement concept drift detection
   - Step 1.2.5: Configure anomaly detection for model outputs
   - Step 1.2.6: Monitor model confidence scores and uncertainty

2. **Implement Prompt Injection Detection**
   - Step 1.2.7: Deploy input validation for AI prompts
   - Step 1.2.8: Implement pattern matching for known injection techniques
   - Step 1.2.9: Configure behavioral analysis for suspicious prompts
   - Step 1.2.10: Deploy output filtering for sensitive information
   - Step 1.2.11: Implement jailbreak attempt detection
   - Step 1.2.12: Configure alerting for prompt injection indicators

3. **Deploy Data Leakage Monitoring**
   - Step 1.2.13: Monitor AI outputs for PII and sensitive data
   - Step 1.2.14: Implement pattern matching for data exfiltration
   - Step 1.2.15: Configure alerts for unusual data access patterns
   - Step 1.2.16: Monitor for training data memorization
   - Step 1.2.17: Detect membership inference attempts
   - Step 1.2.18: Monitor for model inversion attacks

4. **Implement Adversarial Input Detection**
   - Step 1.2.19: Deploy adversarial example detection
   - Step 1.2.20: Implement input sanitization and validation
   - Step 1.2.21: Configure anomaly detection for input distributions
   - Step 1.2.22: Monitor for evasion attack patterns
   - Step 1.2.23: Detect input perturbations and noise
   - Step 1.2.24: Alert on suspicious input characteristics

5. **Monitor AI API and Service Usage**
   - Step 1.2.25: Implement API rate limiting and monitoring
   - Step 1.2.26: Track API authentication and authorization
   - Step 1.2.27: Monitor for API abuse patterns
   - Step 1.2.28: Detect credential theft and misuse
   - Step 1.2.29: Monitor for resource exhaustion attacks
   - Step 1.2.30: Track API error rates and anomalies

6. **Deploy Deepfake Detection**
   - Step 1.2.31: Implement deepfake detection for video communications
   - Step 1.2.32: Deploy voice authentication and verification
   - Step 1.2.33: Monitor for synthetic media in communications
   - Step 1.2.34: Implement biometric liveness detection
   - Step 1.2.35: Configure alerts for suspected deepfakes
   - Step 1.2.36: Validate authenticity of high-risk communications

7. **Integrate AI Threat Intelligence**
   - Step 1.2.37: Subscribe to AI-specific threat intelligence feeds
   - Step 1.2.38: Monitor AI Incident Database (incidentdatabase.ai)
   - Step 1.2.39: Track OWASP Top 10 for LLM Applications
   - Step 1.2.40: Monitor MITRE ATLAS for AI attack techniques
   - Step 1.2.41: Subscribe to AI security research publications
   - Step 1.2.42: Integrate AI threat intelligence into SIEM

**Deliverables**:
- AI monitoring infrastructure
- Prompt injection detection system
- Adversarial input detection
- Deepfake detection capabilities
- AI threat intelligence integration

**Success Criteria**:
- All AI systems monitored for security threats
- Automated detection for common AI attacks
- Threat intelligence integrated
- Alerting operational

---

### 1.3 AI Incident Response Team and Training

**Objective**: Establish specialized AI incident response capabilities

**Exact Steps**:

1. **Establish AI Security Roles**
   - Step 1.3.1: Designate AI Security Lead
   - Step 1.3.2: Identify ML Engineers for incident response
   - Step 1.3.3: Designate Data Scientists for model analysis
   - Step 1.3.4: Identify AI Ethics and Governance representatives
   - Step 1.3.5: Establish AI Forensics specialists
   - Step 1.3.6: Document AI incident response roles

2. **Conduct AI Security Training**
   - Step 1.3.7: Train IR team on AI attack vectors
   - Step 1.3.8: Conduct prompt injection attack training
   - Step 1.3.9: Train on adversarial ML techniques
   - Step 1.3.10: Conduct deepfake detection training
   - Step 1.3.11: Train on AI model forensics
   - Step 1.3.12: Conduct OWASP COMPASS framework training

3. **Develop AI Incident Playbooks**
   - Step 1.3.13: Create prompt injection response playbook
   - Step 1.3.14: Develop data leakage response playbook
   - Step 1.3.15: Create model poisoning response playbook
   - Step 1.3.16: Develop deepfake incident playbook
   - Step 1.3.17: Create shadow AI discovery playbook
   - Step 1.3.18: Develop autonomous AI attack playbook

4. **Conduct AI Incident Exercises**
   - Step 1.3.19: Develop AI incident scenarios
   - Step 1.3.20: Conduct prompt injection tabletop exercise
   - Step 1.3.21: Simulate deepfake attack scenario
   - Step 1.3.22: Exercise model poisoning response
   - Step 1.3.23: Simulate autonomous AI attack
   - Step 1.3.24: Document exercise lessons learned

**Deliverables**:
- AI incident response team roster
- AI security training curriculum
- AI incident playbooks
- Exercise scenarios and results

**Success Criteria**:
- AI-specialized IR team established
- Team trained on AI threats
- Playbooks developed and tested
- Exercises completed successfully

---

### 1.4 AI Governance and Policy Framework

**Objective**: Establish governance framework for AI security

**Exact Steps**:

1. **Develop AI Security Policies**
   - Step 1.4.1: Create AI acceptable use policy
   - Step 1.4.2: Develop AI data handling policy
   - Step 1.4.3: Establish AI model development security standards
   - Step 1.4.4: Create AI deployment approval process
   - Step 1.4.5: Develop AI incident reporting policy
   - Step 1.4.6: Establish AI third-party risk management policy

2. **Implement AI Access Controls**
   - Step 1.4.7: Define role-based access for AI systems
   - Step 1.4.8: Implement least privilege for AI model access
   - Step 1.4.9: Establish AI API authentication requirements
   - Step 1.4.10: Configure multi-factor authentication for AI platforms
   - Step 1.4.11: Implement segregation of duties for AI operations
   - Step 1.4.12: Document AI access control procedures

3. **Establish AI Change Management**
   - Step 1.4.13: Create AI model change approval process
   - Step 1.4.14: Implement version control for AI models
   - Step 1.4.15: Establish testing requirements for model updates
   - Step 1.4.16: Define rollback procedures for AI deployments
   - Step 1.4.17: Implement change documentation requirements
   - Step 1.4.18: Configure automated change notifications

4. **Implement AI Compliance Framework**
   - Step 1.4.19: Identify applicable AI regulations (EU AI Act, etc.)
   - Step 1.4.20: Establish AI risk assessment procedures
   - Step 1.4.21: Implement AI impact assessments
   - Step 1.4.22: Create AI audit trail requirements
   - Step 1.4.23: Establish AI transparency and explainability standards
   - Step 1.4.24: Document compliance procedures

**Deliverables**:
- AI security policy framework
- Access control procedures
- Change management process
- Compliance documentation

**Success Criteria**:
- Comprehensive AI policies established
- Access controls implemented
- Change management operational
- Compliance framework documented

---

### 1.5 AI Forensics and Evidence Collection Capabilities

**Objective**: Establish specialized capabilities for AI incident investigation

**Exact Steps**:

1. **Deploy Model Versioning and Logging**
   - Step 1.5.1: Implement comprehensive model versioning
   - Step 1.5.2: Configure model lineage tracking
   - Step 1.5.3: Enable detailed inference logging
   - Step 1.5.4: Implement training data provenance tracking
   - Step 1.5.5: Configure parameter and weight versioning
   - Step 1.5.6: Establish model artifact retention policies

2. **Implement AI Audit Logging**
   - Step 1.5.7: Log all AI model access and queries
   - Step 1.5.8: Capture input prompts and outputs
   - Step 1.5.9: Log model predictions and confidence scores
   - Step 1.5.10: Track model performance metrics
   - Step 1.5.11: Log API calls and authentication events
   - Step 1.5.12: Implement tamper-evident logging

3. **Establish Model Snapshot Procedures**
   - Step 1.5.13: Create automated model snapshot procedures
   - Step 1.5.14: Implement snapshot integrity verification
   - Step 1.5.15: Establish snapshot retention policies
   - Step 1.5.16: Configure snapshot storage security
   - Step 1.5.17: Document snapshot restoration procedures
   - Step 1.5.18: Test snapshot recovery capabilities

4. **Deploy AI Forensic Tools**
   - Step 1.5.19: Acquire model analysis and inspection tools
   - Step 1.5.20: Deploy adversarial example detection tools
   - Step 1.5.21: Implement model extraction detection
   - Step 1.5.22: Deploy data poisoning detection tools
   - Step 1.5.23: Acquire prompt injection analysis tools
   - Step 1.5.24: Implement model interpretability tools

**Deliverables**:
- Model versioning system
- AI audit logging infrastructure
- Model snapshot procedures
- AI forensic toolkit

**Success Criteria**:
- Complete model lineage tracking
- Comprehensive audit logging
- Snapshot procedures tested
- Forensic tools operational

---

## Phase 2: Detection & Analysis of AI Incidents

### 2.1 AI Incident Detection and Declaration

**Objective**: Identify and formally declare AI security incidents

**Exact Steps**:

1. **Detect AI Security Anomalies**
   - Step 2.1.1: Monitor for unusual AI model behavior
   - Step 2.1.2: Detect significant model performance degradation
   - Step 2.1.3: Identify unexpected model outputs or predictions
   - Step 2.1.4: Detect data drift or distribution shifts
   - Step 2.1.5: Identify prompt injection attempts
   - Step 2.1.6: Detect adversarial input patterns
   - Step 2.1.7: Monitor for unusual API usage patterns
   - Step 2.1.8: Identify potential data leakage in outputs

2. **Triage AI Security Events**
   - Step 2.1.9: Assess severity of detected anomaly
   - Step 2.1.10: Determine if anomaly represents security incident
   - Step 2.1.11: Rule out model errors vs. security events
   - Step 2.1.12: Assess potential impact on users and operations
   - Step 2.1.13: Determine if incident meets AI-S1/S2 criteria
   - Step 2.1.14: Document initial triage findings

3. **Categorize AI Incident Type**
   - Step 2.1.15: Determine incident category (see AI Incident Categories section)
   - Step 2.1.16: Identify if AI is attack vector or attack target
   - Step 2.1.17: Assess which OWASP COMPASS profile applies
   - Step 2.1.18: Map incident to MITRE ATLAS techniques
   - Step 2.1.19: Determine if incident involves multiple AI systems
   - Step 2.1.20: Document incident categorization

4. **Declare AI Incident**
   - Step 2.1.21: Formally declare AI security incident
   - Step 2.1.22: Assign AI incident severity level
   - Step 2.1.23: Create incident case in case management system
   - Step 2.1.24: Notify AI Security Lead and Incident Manager
   - Step 2.1.25: Activate AI incident response team
   - Step 2.1.26: Establish AI incident war room

5. **Report AI Incident**
   - Step 2.1.27: Report to CISA (all AI incidents)
   - Step 2.1.28: Report to OMB if major incident
   - Step 2.1.29: Submit to AI Incident Database if appropriate
   - Step 2.1.30: Notify AI system stakeholders
   - Step 2.1.31: Brief executive leadership on AI incident
   - Step 2.1.32: Document all notifications

**Deliverables**:
- AI incident declaration
- Incident categorization
- Stakeholder notifications
- CISA/OMB reports

**Success Criteria**:
- Incident detected and triaged
- Severity appropriately assigned
- Team activated
- Required notifications completed

---

### 2.2 AI Incident Scope Determination (OBSERVE + ORIENT)

**Objective**: Apply OODA Loop to assess AI incident scope and context

**Exact Steps - OBSERVE Phase**:

1. **Observe AI System Behavior**
   - Step 2.2.1: Collect current AI model performance metrics
   - Step 2.2.2: Analyze recent model predictions and outputs
   - Step 2.2.3: Review input data distributions
   - Step 2.2.4: Examine model confidence scores
   - Step 2.2.5: Analyze error rates and anomalies
   - Step 2.2.6: Document observed behavioral changes

2. **Observe Attack Surface**
   - Step 2.2.7: Identify all potentially affected AI systems
   - Step 2.2.8: Map data flows to/from affected systems
   - Step 2.2.9: Identify exposed AI APIs and endpoints
   - Step 2.2.10: Assess user access patterns to AI systems
   - Step 2.2.11: Review third-party AI service integrations
   - Step 2.2.12: Document attack surface assessment

3. **Observe Indicators of Compromise**
   - Step 2.2.13: Identify unusual prompts or inputs
   - Step 2.2.14: Detect adversarial perturbations in inputs
   - Step 2.2.15: Identify suspicious API access patterns
   - Step 2.2.16: Detect unauthorized model access
   - Step 2.2.17: Identify data exfiltration indicators
   - Step 2.2.18: Document all IOCs

**Exact Steps - ORIENT Phase**:

4. **Orient with Threat Intelligence**
   - Step 2.2.19: Review MITRE ATLAS for matching techniques
   - Step 2.2.20: Check OWASP Top 10 for LLM Applications
   - Step 2.2.21: Review AI Incident Database for similar incidents
   - Step 2.2.22: Consult AI threat intelligence feeds
   - Step 2.2.23: Review recent AI security research
   - Step 2.2.24: Map incident to known AI attack patterns

5. **Orient with AI System Context**
   - Step 2.2.25: Review AI system architecture and design
   - Step 2.2.26: Assess model training data and provenance
   - Step 2.2.27: Review model update and deployment history
   - Step 2.2.28: Analyze system dependencies and integrations
   - Step 2.2.29: Review access controls and permissions
   - Step 2.2.30: Document system context assessment

6. **Orient with Business Impact**
   - Step 2.2.31: Assess impact on business operations
   - Step 2.2.32: Evaluate impact on users and customers
   - Step 2.2.33: Assess safety and ethical implications
   - Step 2.2.34: Evaluate regulatory and compliance impact
   - Step 2.2.35: Assess reputational risk
   - Step 2.2.36: Document business impact assessment

7. **Define Investigation Scope**
   - Step 2.2.37: Determine investigation time window
   - Step 2.2.38: Identify all AI systems requiring investigation
   - Step 2.2.39: Identify all data sources to examine
   - Step 2.2.40: Define model versions to analyze
   - Step 2.2.41: Identify users and accounts to investigate
   - Step 2.2.42: Document investigation scope

**Deliverables**:
- AI system behavior analysis
- Attack surface assessment
- Threat intelligence correlation
- Business impact assessment
- Investigation scope document

**Success Criteria**:
- Complete understanding of AI system state
- Incident mapped to known attack patterns
- Business impact assessed
- Investigation scope defined

---

### 2.3 AI-Specific Evidence Collection

**Objective**: Collect specialized evidence from AI systems

**Exact Steps**:

1. **Collect Model Artifacts**
   - Step 2.3.1: Snapshot current model weights and parameters
   - Step 2.3.2: Collect model configuration files
   - Step 2.3.3: Preserve model training history
   - Step 2.3.4: Collect model performance metrics
   - Step 2.3.5: Snapshot model serving infrastructure state
   - Step 2.3.6: Calculate cryptographic hashes of model artifacts
   - Step 2.3.7: Document model artifact collection

2. **Collect Training and Inference Data**
   - Step 2.3.8: Preserve training dataset snapshots
   - Step 2.3.9: Collect recent inference inputs and outputs
   - Step 2.3.10: Preserve data preprocessing pipelines
   - Step 2.3.11: Collect feature engineering code
   - Step 2.3.12: Preserve data augmentation procedures
   - Step 2.3.13: Document data collection procedures

3. **Collect AI System Logs**
   - Step 2.3.14: Extract model inference logs
   - Step 2.3.15: Collect API access logs
   - Step 2.3.16: Preserve authentication and authorization logs
   - Step 2.3.17: Collect model update and deployment logs
   - Step 2.3.18: Extract performance monitoring logs
   - Step 2.3.19: Collect error and exception logs
   - Step 2.3.20: Document log collection

4. **Collect Prompt and Input Data**
   - Step 2.3.21: Preserve all user prompts during incident timeframe
   - Step 2.3.22: Collect suspicious or malicious prompts
   - Step 2.3.23: Preserve prompt injection attempts
   - Step 2.3.24: Collect adversarial input examples
   - Step 2.3.25: Preserve input validation logs
   - Step 2.3.26: Document prompt collection

5. **Collect Infrastructure Evidence**
   - Step 2.3.27: Snapshot cloud AI service configurations
   - Step 2.3.28: Collect container and orchestration logs
   - Step 2.3.29: Preserve GPU/compute resource logs
   - Step 2.3.30: Collect network traffic to/from AI services
   - Step 2.3.31: Preserve storage access logs
   - Step 2.3.32: Document infrastructure evidence

6. **Preserve Evidence Integrity**
   - Step 2.3.33: Calculate hashes for all collected evidence
   - Step 2.3.34: Implement chain of custody for AI artifacts
   - Step 2.3.35: Store evidence in secure repository
   - Step 2.3.36: Encrypt sensitive AI data
   - Step 2.3.37: Create backup copies of critical evidence
   - Step 2.3.38: Document evidence preservation

**Deliverables**:
- Model artifact snapshots
- Training and inference data
- Comprehensive AI system logs
- Prompt and input data
- Infrastructure evidence
- Chain of custody documentation

**Success Criteria**:
- All relevant AI evidence collected
- Model state preserved
- Evidence integrity maintained
- Chain of custody documented

---

### 2.4 AI Incident Technical Analysis

**Objective**: Conduct specialized analysis of AI security incident

**Exact Steps**:

1. **Analyze Model Behavior Changes**
   - Step 2.4.1: Compare current vs. baseline model performance
   - Step 2.4.2: Analyze prediction accuracy changes
   - Step 2.4.3: Examine confidence score distributions
   - Step 2.4.4: Identify anomalous output patterns
   - Step 2.4.5: Analyze error rate changes
   - Step 2.4.6: Document behavioral analysis findings

2. **Analyze for Prompt Injection**
   - Step 2.4.7: Review collected prompts for injection patterns
   - Step 2.4.8: Identify prompt manipulation techniques
   - Step 2.4.9: Analyze jailbreak attempts
   - Step 2.4.10: Examine prompt leaking attempts
   - Step 2.4.11: Identify indirect prompt injection via data
   - Step 2.4.12: Document prompt injection analysis

3. **Analyze for Data Poisoning**
   - Step 2.4.13: Examine training data for anomalies
   - Step 2.4.14: Identify suspicious data samples
   - Step 2.4.15: Analyze data distribution changes
   - Step 2.4.16: Detect backdoor triggers in data
   - Step 2.4.17: Examine feedback loop poisoning
   - Step 2.4.18: Document data poisoning findings

4. **Analyze for Adversarial Attacks**
   - Step 2.4.19: Identify adversarial perturbations in inputs
   - Step 2.4.20: Analyze evasion attack patterns
   - Step 2.4.21: Detect model inversion attempts
   - Step 2.4.22: Identify membership inference attacks
   - Step 2.4.23: Analyze model extraction attempts
   - Step 2.4.24: Document adversarial attack analysis

5. **Analyze for Data Leakage**
   - Step 2.4.25: Review outputs for sensitive data exposure
   - Step 2.4.26: Identify training data memorization
   - Step 2.4.27: Detect PII in model responses
   - Step 2.4.28: Analyze cross-tenant data leakage
   - Step 2.4.29: Identify unintended information disclosure
   - Step 2.4.30: Document data leakage findings

6. **Analyze Model Integrity**
   - Step 2.4.31: Compare current model with known-good version
   - Step 2.4.32: Verify model weight integrity
   - Step 2.4.33: Detect unauthorized model modifications
   - Step 2.4.34: Analyze model supply chain integrity
   - Step 2.4.35: Verify model provenance
   - Step 2.4.36: Document integrity analysis

7. **Map to MITRE ATLAS Framework**
   - Step 2.4.37: Identify ATLAS tactics used
   - Step 2.4.38: Map observed techniques to ATLAS
   - Step 2.4.39: Identify attack procedures
   - Step 2.4.40: Document ATLAS mapping
   - Step 2.4.41: Identify gaps in defenses
   - Step 2.4.42: Create ATLAS-based threat profile

8. **Assess Root Cause**
   - Step 2.4.43: Determine initial attack vector
   - Step 2.4.44: Identify vulnerabilities exploited
   - Step 2.4.45: Assess contributing factors
   - Step 2.4.46: Determine if attack was targeted or opportunistic
   - Step 2.4.47: Assess attacker sophistication
   - Step 2.4.48: Document root cause analysis

**Deliverables**:
- Model behavior analysis report
- Prompt injection analysis
- Data poisoning assessment
- Adversarial attack analysis
- Data leakage report
- Model integrity assessment
- MITRE ATLAS mapping
- Root cause analysis

**Success Criteria**:
- Attack methodology fully understood
- All attack vectors identified
- Root cause determined
- ATLAS framework mapping complete

---

## Phase 3: Containment of AI Incidents

### 3.1 AI Incident Containment Strategy (DECIDE Phase)

**Objective**: Apply OODA Loop DECIDE phase to determine containment approach

**Exact Steps**:

1. **Decide on Containment Priorities**
   - Step 3.1.1: Assess risk of continued AI system operation
   - Step 3.1.2: Evaluate impact of AI system shutdown
   - Step 3.1.3: Determine if partial containment is viable
   - Step 3.1.4: Assess risk of alerting adversary
   - Step 3.1.5: Evaluate need for coordinated containment
   - Step 3.1.6: Prioritize user safety vs. business continuity

2. **Decide on Containment Methods**
   - Step 3.1.7: Determine if model should be taken offline
   - Step 3.1.8: Assess viability of input filtering
   - Step 3.1.9: Evaluate output sanitization options
   - Step 3.1.10: Determine if rollback to previous version is needed
   - Step 3.1.11: Assess need for rate limiting or throttling
   - Step 3.1.12: Evaluate API access restriction options

3. **Assess Containment Trade-offs**
   - Step 3.1.13: Evaluate operational impact of each option
   - Step 3.1.14: Assess user experience implications
   - Step 3.1.15: Evaluate regulatory and compliance considerations
   - Step 3.1.16: Assess safety and ethical implications
   - Step 3.1.17: Evaluate reputational impact
   - Step 3.1.18: Document trade-off analysis

4. **Develop Containment Plan**
   - Step 3.1.19: Define specific containment actions
   - Step 3.1.20: Establish containment timeline
   - Step 3.1.21: Identify required approvals
   - Step 3.1.22: Define success criteria
   - Step 3.1.23: Prepare rollback procedures
   - Step 3.1.24: Document containment plan

5. **Obtain Approvals**
   - Step 3.1.25: Brief Incident Manager on plan
   - Step 3.1.26: Obtain AI Security Lead approval
   - Step 3.1.27: Coordinate with business stakeholders
   - Step 3.1.28: Obtain executive approval if needed
   - Step 3.1.29: Document all approvals
   - Step 3.1.30: Prepare stakeholder communications

**Deliverables**:
- Containment strategy document
- Trade-off analysis
- Containment plan
- Approval documentation
- Stakeholder communication plan

**Success Criteria**:
- Comprehensive containment strategy developed
- Trade-offs assessed and documented
- Plan approved by stakeholders
- Ready for execution

---

### 3.2 Execute AI Containment Actions (ACT Phase)

**Objective**: Implement containment measures for AI systems

**Exact Steps**:

1. **Implement Model-Level Containment**
   - Step 3.2.1: Take compromised model offline if determined necessary
   - Step 3.2.2: Rollback to previous known-good model version
   - Step 3.2.3: Implement input validation and sanitization
   - Step 3.2.4: Deploy output filtering for sensitive data
   - Step 3.2.5: Implement additional guardrails and safety checks
   - Step 3.2.6: Document model containment actions

2. **Implement Access-Based Containment**
   - Step 3.2.7: Disable compromised user accounts
   - Step 3.2.8: Revoke API keys and access tokens
   - Step 3.2.9: Implement IP-based access restrictions
   - Step 3.2.10: Enforce stricter authentication requirements
   - Step 3.2.11: Implement rate limiting on AI APIs
   - Step 3.2.12: Document access containment actions

3. **Implement Data-Level Containment**
   - Step 3.2.13: Isolate potentially poisoned training data
   - Step 3.2.14: Suspend data ingestion pipelines if compromised
   - Step 3.2.15: Implement additional data validation
   - Step 3.2.16: Quarantine suspicious data samples
   - Step 3.2.17: Prevent further data exfiltration
   - Step 3.2.18: Document data containment actions

4. **Implement Infrastructure Containment**
   - Step 3.2.19: Isolate compromised AI infrastructure
   - Step 3.2.20: Implement network segmentation
   - Step 3.2.21: Restrict cloud AI service access
   - Step 3.2.22: Disable compromised integrations
   - Step 3.2.23: Implement enhanced monitoring
   - Step 3.2.24: Document infrastructure containment

5. **Implement User Communication**
   - Step 3.2.25: Notify affected users of incident
   - Step 3.2.26: Provide guidance on AI system status
   - Step 3.2.27: Communicate alternative procedures
   - Step 3.2.28: Establish user support channels
   - Step 3.2.29: Document user communications
   - Step 3.2.30: Monitor user feedback

6. **Verify Containment Effectiveness**
   - Step 3.2.31: Monitor for continued malicious activity
   - Step 3.2.32: Verify model behavior normalized
   - Step 3.2.33: Confirm no new data leakage
   - Step 3.2.34: Validate access restrictions effective
   - Step 3.2.35: Assess completeness of containment
   - Step 3.2.36: Document verification results

**Deliverables**:
- Containment action log
- Model rollback documentation
- Access restriction records
- User communications
- Verification results

**Success Criteria**:
- All containment actions executed
- Malicious activity stopped
- Model behavior stabilized
- Users informed
- Containment verified

---

## Phase 4: Eradication & Recovery

### 4.1 AI System Eradication

**Objective**: Completely remove adversary access and malicious modifications

**Exact Steps**:

1. **Eradicate Model Compromises**
   - Step 4.1.1: Identify all compromised model versions
   - Step 4.1.2: Remove poisoned or backdoored models
   - Step 4.1.3: Delete adversary-modified model weights
   - Step 4.1.4: Remove malicious model configurations
   - Step 4.1.5: Purge compromised model artifacts
   - Step 4.1.6: Document model eradication

2. **Eradicate Data Compromises**
   - Step 4.1.7: Identify and remove poisoned training data
   - Step 4.1.8: Purge adversarial samples from datasets
   - Step 4.1.9: Remove backdoor triggers from data
   - Step 4.1.10: Clean data preprocessing pipelines
   - Step 4.1.11: Verify data integrity
   - Step 4.1.12: Document data eradication

3. **Eradicate Access Compromises**
   - Step 4.1.13: Remove adversary-created accounts
   - Step 4.1.14: Revoke all compromised credentials
   - Step 4.1.15: Rotate all API keys and tokens
   - Step 4.1.16: Remove unauthorized integrations
   - Step 4.1.17: Purge adversary access artifacts
   - Step 4.1.18: Document access eradication

4. **Rebuild AI Systems**
   - Step 4.1.19: Retrain models from clean data if necessary
   - Step 4.1.20: Rebuild AI infrastructure from known-good state
   - Step 4.1.21: Redeploy models with enhanced security
   - Step 4.1.22: Implement additional security controls
   - Step 4.1.23: Verify system integrity
   - Step 4.1.24: Document rebuild procedures

5. **Patch Vulnerabilities**
   - Step 4.1.25: Identify vulnerabilities exploited in incident
   - Step 4.1.26: Implement input validation improvements
   - Step 4.1.27: Enhance output filtering
   - Step 4.1.28: Strengthen access controls
   - Step 4.1.29: Deploy additional monitoring
   - Step 4.1.30: Document vulnerability remediation

**Deliverables**:
- Model eradication documentation
- Data cleaning procedures
- System rebuild documentation
- Vulnerability remediation report

**Success Criteria**:
- All compromises removed
- Systems rebuilt securely
- Vulnerabilities patched
- Integrity verified

---

### 4.2 AI System Recovery

**Objective**: Restore AI services to normal operation

**Exact Steps**:

1. **Validate Model Integrity**
   - Step 4.2.1: Verify model weights and parameters
   - Step 4.2.2: Validate model performance metrics
   - Step 4.2.3: Test model outputs for correctness
   - Step 4.2.4: Verify absence of backdoors
   - Step 4.2.5: Confirm model provenance
   - Step 4.2.6: Document integrity validation

2. **Restore AI Services**
   - Step 4.2.7: Gradually restore AI model availability
   - Step 4.2.8: Implement canary deployment for testing
   - Step 4.2.9: Monitor model behavior during restoration
   - Step 4.2.10: Restore API access incrementally
   - Step 4.2.11: Validate service functionality
   - Step 4.2.12: Document service restoration

3. **Restore User Access**
   - Step 4.2.13: Re-enable user accounts with new credentials
   - Step 4.2.14: Issue new API keys to legitimate users
   - Step 4.2.15: Communicate restoration to users
   - Step 4.2.16: Provide user training on security enhancements
   - Step 4.2.17: Monitor for access issues
   - Step 4.2.18: Document access restoration

4. **Implement Enhanced Monitoring**
   - Step 4.2.19: Deploy additional model monitoring
   - Step 4.2.20: Implement enhanced input validation
   - Step 4.2.21: Configure stricter output filtering
   - Step 4.2.22: Deploy behavioral analytics
   - Step 4.2.23: Implement continuous model validation
   - Step 4.2.24: Document enhanced monitoring

5. **Validate Recovery**
   - Step 4.2.25: Verify all AI services operational
   - Step 4.2.26: Confirm model performance acceptable
   - Step 4.2.27: Validate user access restored
   - Step 4.2.28: Verify no signs of re-compromise
   - Step 4.2.29: Obtain stakeholder confirmation
   - Step 4.2.30: Document recovery validation

**Deliverables**:
- Model validation results
- Service restoration documentation
- Enhanced monitoring procedures
- Recovery validation report

**Success Criteria**:
- AI services fully restored
- Model integrity validated
- Enhanced security in place
- No signs of re-compromise

---

## Phase 5: Post-Incident Activities

### 5.1 AI Incident Post-Mortem

**Objective**: Conduct comprehensive review of AI incident

**Exact Steps**:

1. **Conduct AI Incident Review**
   - Step 5.1.1: Schedule post-incident review meeting
   - Step 5.1.2: Invite AI incident response team
   - Step 5.1.3: Include ML engineers and data scientists
   - Step 5.1.4: Invite AI governance representatives
   - Step 5.1.5: Present complete incident timeline
   - Step 5.1.6: Review detection and response effectiveness

2. **Assess AI-Specific Lessons Learned**
   - Step 5.1.7: Evaluate AI monitoring effectiveness
   - Step 5.1.8: Assess model security controls
   - Step 5.1.9: Review data protection measures
   - Step 5.1.10: Evaluate prompt injection defenses
   - Step 5.1.11: Assess adversarial robustness
   - Step 5.1.12: Document AI-specific lessons

3. **Update AI Threat Intelligence**
   - Step 5.1.13: Document new AI attack techniques observed
   - Step 5.1.14: Update MITRE ATLAS mapping
   - Step 5.1.15: Contribute to AI Incident Database
   - Step 5.1.16: Share intelligence with AI security community
   - Step 5.1.17: Update internal threat profiles
   - Step 5.1.18: Document threat intelligence updates

4. **Develop AI Security Improvements**
   - Step 5.1.19: Identify model security enhancements
   - Step 5.1.20: Recommend data protection improvements
   - Step 5.1.21: Propose monitoring enhancements
   - Step 5.1.22: Suggest governance improvements
   - Step 5.1.23: Recommend training updates
   - Step 5.1.24: Document improvement recommendations

**Deliverables**:
- AI incident post-mortem report
- Lessons learned documentation
- Threat intelligence updates
- Improvement recommendations

**Success Criteria**:
- Comprehensive review completed
- AI-specific lessons documented
- Threat intelligence updated
- Improvements identified

---

### 5.2 Update AI Security Posture (Return to OBSERVE)

**Objective**: Implement improvements and return to OODA Loop observation

**Exact Steps**:

1. **Enhance AI Detection Capabilities**
   - Step 5.2.1: Implement new detection rules from incident
   - Step 5.2.2: Enhance prompt injection detection
   - Step 5.2.3: Improve adversarial input detection
   - Step 5.2.4: Deploy additional model monitoring
   - Step 5.2.5: Enhance data leakage detection
   - Step 5.2.6: Document detection enhancements

2. **Strengthen AI Security Controls**
   - Step 5.2.7: Implement enhanced input validation
   - Step 5.2.8: Strengthen output filtering
   - Step 5.2.9: Improve access controls
   - Step 5.2.10: Enhance model versioning and integrity checks
   - Step 5.2.11: Implement additional guardrails
   - Step 5.2.12: Document control enhancements

3. **Update AI Governance**
   - Step 5.2.13: Revise AI security policies
   - Step 5.2.14: Update AI risk assessment procedures
   - Step 5.2.15: Enhance AI approval processes
   - Step 5.2.16: Strengthen compliance requirements
   - Step 5.2.17: Update AI incident playbooks
   - Step 5.2.18: Document governance updates

4. **Conduct AI Security Training**
   - Step 5.2.19: Develop training on new attack techniques
   - Step 5.2.20: Train developers on secure AI practices
   - Step 5.2.21: Educate users on AI security
   - Step 5.2.22: Update IR team on AI threats
   - Step 5.2.23: Conduct tabletop exercise with incident scenario
   - Step 5.2.24: Document training completion

5. **Return to Continuous Monitoring (OBSERVE)**
   - Step 5.2.25: Resume normal AI threat monitoring
   - Step 5.2.26: Implement enhanced OODA loop cycle
   - Step 5.2.27: Monitor for similar attack patterns
   - Step 5.2.28: Track effectiveness of improvements
   - Step 5.2.29: Maintain heightened vigilance
   - Step 5.2.30: Document return to steady state

**Deliverables**:
- Enhanced detection capabilities
- Strengthened security controls
- Updated governance framework
- Training completion records
- Continuous monitoring procedures

**Success Criteria**:
- All improvements implemented
- Controls strengthened
- Team trained
- Continuous monitoring resumed
- OODA loop cycle complete

---

## AI-Specific Incident Categories

### Category 1: AI-Powered Attacks (AI as Attack Vector)

**Description**: Adversaries using AI to enhance attacks against the organization

**Incident Types**:

1. **AI-Generated Phishing**
   - LLM-generated spear-phishing emails
   - Contextually aware social engineering
   - Multilingual phishing campaigns
   - Personalized attack content

2. **Deepfake Attacks**
   - Executive impersonation (voice/video)
   - Synthetic identity fraud
   - Deepfake extortion
   - Biometric spoofing

3. **Autonomous AI Attacks**
   - Self-directed penetration testing
   - Adaptive evasion techniques
   - Automated lateral movement
   - AI-driven data exfiltration

4. **AI-Enhanced Reconnaissance**
   - Automated OSINT collection
   - Pattern-based vulnerability discovery
   - Social engineering target profiling
   - Network mapping and analysis

**Response Priorities**:
- Detect AI-generated content
- Implement deepfake detection
- Monitor for autonomous attack behaviors
- Enhance user awareness training

---

### Category 2: Prompt Injection and Manipulation

**Description**: Attacks targeting AI systems through malicious inputs

**Incident Types**:

1. **Direct Prompt Injection**
   - Malicious instructions in user prompts
   - System prompt override attempts
   - Jailbreaking and guardrail bypass
   - Prompt leaking attacks

2. **Indirect Prompt Injection**
   - Poisoned data sources
   - Malicious content in retrieved documents
   - Compromised RAG (Retrieval-Augmented Generation) sources
   - Third-party data manipulation

3. **Cross-Prompt Injection**
   - Multi-user system manipulation
   - Persistent prompt injection
   - Context pollution attacks
   - Session hijacking via prompts

**Response Priorities**:
- Implement robust input validation
- Deploy prompt injection detection
- Sanitize external data sources
- Implement output filtering

---

### Category 3: Model Poisoning and Manipulation

**Description**: Attacks targeting AI model integrity

**Incident Types**:

1. **Training Data Poisoning**
   - Adversarial sample injection
   - Backdoor trigger insertion
   - Bias amplification
   - Label manipulation

2. **Model Backdooring**
   - Trigger-based malicious behavior
   - Conditional model compromise
   - Stealth backdoor insertion
   - Supply chain model compromise

3. **Feedback Loop Poisoning**
   - Malicious user feedback
   - Reinforcement learning manipulation
   - Online learning exploitation
   - Active learning poisoning

**Response Priorities**:
- Implement data validation
- Monitor model behavior changes
- Detect backdoor triggers
- Secure model supply chain

---

### Category 4: Data Leakage and Privacy Violations

**Description**: Unauthorized disclosure of sensitive information through AI systems

**Incident Types**:

1. **Training Data Extraction**
   - Model inversion attacks
   - Membership inference
   - Training data memorization
   - Reconstruction attacks

2. **Sensitive Information Disclosure**
   - PII in model outputs
   - Confidential data leakage
   - Cross-tenant data exposure
   - Unintended information revelation

3. **Model Extraction**
   - API-based model theft
   - Architecture reverse engineering
   - Knowledge distillation
   - Model replication

**Response Priorities**:
- Implement output sanitization
- Deploy data leakage prevention
- Monitor for extraction attempts
- Implement differential privacy

---

### Category 5: Adversarial Machine Learning

**Description**: Attacks exploiting AI model vulnerabilities

**Incident Types**:

1. **Evasion Attacks**
   - Adversarial perturbations
   - Input manipulation
   - Misclassification attacks
   - Detection bypass

2. **Model Inversion**
   - Training data reconstruction
   - Feature extraction
   - Attribute inference
   - Privacy violation

3. **Byzantine Attacks**
   - Distributed learning manipulation
   - Federated learning poisoning
   - Consensus manipulation
   - Gradient attacks

**Response Priorities**:
- Implement adversarial detection
- Deploy input sanitization
- Monitor model robustness
- Implement defensive distillation

---

### Category 6: AI Infrastructure Compromise

**Description**: Attacks targeting AI system infrastructure

**Incident Types**:

1. **Cloud AI Service Compromise**
   - Stolen cloud credentials
   - LLM hijacking
   - Resource abuse
   - API key theft

2. **Model Serving Infrastructure**
   - Compromised inference endpoints
   - Container escape
   - Orchestration exploitation
   - GPU resource hijacking

3. **ML Pipeline Compromise**
   - Training pipeline manipulation
   - Deployment pipeline compromise
   - CI/CD exploitation
   - Model registry compromise

**Response Priorities**:
- Secure cloud AI services
- Implement infrastructure monitoring
- Protect ML pipelines
- Secure model repositories

---

### Category 7: Shadow AI and Governance Violations

**Description**: Unauthorized or unmanaged AI usage

**Incident Types**:

1. **Unauthorized AI Tools**
   - Unapproved AI service usage
   - Sensitive data exposure to third-party AI
   - Compliance violations
   - Unmanaged AI deployments

2. **AI Policy Violations**
   - Inappropriate AI usage
   - Ethical guideline violations
   - Regulatory non-compliance
   - Unapproved AI experiments

3. **AI Supply Chain Risks**
   - Untrusted model sources
   - Unvetted third-party AI services
   - Insecure AI integrations
   - Vendor risk exposure

**Response Priorities**:
- Discover shadow AI
- Enforce AI governance
- Implement approval processes
- Monitor AI usage

---

## Integration with Standard IR Workflow

### Coordination Between AI and Traditional IR

**When to Engage AI-Specific Response**:

1. **AI System Directly Involved**
   - AI model is the target of attack
   - AI system is compromised
   - AI data is affected
   - AI infrastructure is targeted

2. **AI Used as Attack Vector**
   - Deepfake detected in incident
   - AI-generated phishing identified
   - Autonomous attack behaviors observed
   - AI-enhanced techniques detected

3. **AI Context Required**
   - Model behavior analysis needed
   - Prompt injection suspected
   - Data poisoning possible
   - Adversarial attacks detected

**Coordination Procedures**:

1. **Parallel Investigation**
   - Traditional IR investigates infrastructure compromise
   - AI IR team investigates model and data integrity
   - Coordinate findings and timelines
   - Share IOCs and intelligence

2. **Unified Incident Management**
   - Single Incident Manager coordinates both teams
   - Unified case management and documentation
   - Coordinated containment and recovery
   - Joint post-incident review

3. **Escalation Triggers**
   - Escalate to AI IR when AI systems involved
   - Engage traditional IR for infrastructure issues
   - Coordinate for hybrid incidents
   - Unified reporting to CISA and stakeholders

---

## Reporting and Notification

### AI Incident Reporting Requirements

**CISA Reporting**:
- Report all AI security incidents
- Include AI-specific context (model type, attack vector)
- Provide MITRE ATLAS mapping
- Share AI threat intelligence

**OMB Reporting**:
- Report major AI incidents within 1 hour
- Include safety and ethical impact assessment
- Document AI system criticality
- Provide regulatory impact analysis

**AI Incident Database**:
- Consider voluntary submission to incidentdatabase.ai
- Contribute to community knowledge
- Help establish AI incident patterns
- Support AI safety research

**Regulatory Reporting**:
- EU AI Act compliance (if applicable)
- Sector-specific regulations (healthcare, finance, etc.)
- Privacy regulations (GDPR, CCPA, etc.)
- Safety regulations for high-risk AI systems

---

## Conclusion

The AI Security Incident Response Workflow provides a comprehensive framework for responding to the unique challenges of AI/ML security incidents. By integrating the OWASP GenAI COMPASS OODA Loop methodology with traditional NIST incident response practices, organizations can effectively detect, analyze, contain, eradicate, and recover from AI security incidents while continuously improving their AI security posture.

The rapid evolution of AI threats requires continuous adaptation of incident response capabilities. Organizations must maintain vigilance, invest in AI-specific detection and response capabilities, and actively participate in the AI security community to stay ahead of emerging threats.

**Key Success Factors**:
- Comprehensive AI asset inventory and risk classification
- Specialized AI security monitoring and detection
- Trained AI incident response team
- AI-specific playbooks and procedures
- Integration of OODA Loop for continuous improvement
- Active participation in AI threat intelligence sharing
- Regular testing and exercises with AI incident scenarios

**Next Steps**:
1. Implement AI asset inventory and classification
2. Deploy AI-specific monitoring and detection
3. Train incident response team on AI threats
4. Develop and test AI incident playbooks
5. Establish AI governance framework
6. Integrate with threat intelligence community
7. Conduct regular AI incident response exercises

---

**Document End**

